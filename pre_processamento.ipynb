{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO3LJuisYVEX6P7YRbPt3N+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Bibliotecas"],"metadata":{"id":"E9FqVryxdaEw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"z-gIw1xedOn2"},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","import torch\n","import pickle\n","from torchvision import transforms\n","from tqdm import tqdm"]},{"cell_type":"markdown","source":["#Montando o Drive"],"metadata":{"id":"2QWZW-j3gtOe"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Jt0wdSo7gvpv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Diret√≥rios e Defini√ß√µes"],"metadata":{"id":"cgtQfjHidnVe"}},{"cell_type":"code","source":["# Diret√≥rios do KITTI Dataset\n","BASE_PATH = \"/content/drive/My Drive/Sistemas Mecatr√¥nicos/Cibern√©tica e Aprendizagem de M√°quinas/Projeto2/data_odometry_color/dataset/sequences\"\n","BASE_LIDAR = \"/content/drive/My Drive/Sistemas Mecatr√¥nicos/Cibern√©tica e Aprendizagem de M√°quinas/Projeto2/data_odometry_velodyne/dataset/sequences\"\n","BASE_POSES = \"/content/drive/My Drive/Sistemas Mecatr√¥nicos/Cibern√©tica e Aprendizagem de M√°quinas/Projeto2/data_odometry_poses/dataset/poses\"\n","SAVE_PATH = \"/content/drive/My Drive/Sistemas Mecatr√¥nicos/Cibern√©tica e Aprendizagem de M√°quinas/Projeto2/processed_data\"  # Pasta onde os dados ser√£o salvos\n","\n","# Criar a pasta de sa√≠da se n√£o existir\n","os.makedirs(SAVE_PATH, exist_ok=True)\n","\n","# Definir transforma√ß√µes para normalizar imagens\n","image_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize((224, 224)),  # Redimensiona diretamente o tensor\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])"],"metadata":{"id":"w3f3dIYAdfSD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Fun√ß√£o de Load de Imagens"],"metadata":{"id":"MibEQ05pg82L"}},{"cell_type":"code","source":["# Fun√ß√£o para carregar e normalizar imagens RGB\n","def load_images(sequence):\n","    img_path = os.path.join(BASE_PATH, sequence, \"image_2\").replace(\"\\\\\", \"/\")\n","    img_files = sorted(os.listdir(img_path))\n","    images = []\n","\n","    for img_file in tqdm(img_files, desc=f\"Carregando imagens da sequ√™ncia {sequence}\"):\n","        img = cv2.imread(os.path.join(img_path, img_file).replace(\"\\\\\", \"/\"))\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        img = image_transform(img)  # Normaliza a imagem\n","        images.append(img)\n","\n","    return torch.stack(images)  # Retorna tensor (N, C, H, W)"],"metadata":{"id":"v6uuCkgPdrvg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Fun√ß√£o de Load de dados do LiDAR"],"metadata":{"id":"Kn2C2rvGhCCk"}},{"cell_type":"code","source":["# Fun√ß√£o para carregar e normalizar dados LiDAR\n","def load_lidar(sequence):\n","    lidar_path = os.path.join(BASE_LIDAR, sequence, \"velodyne\").replace(\"\\\\\", \"/\")\n","    lidar_files = sorted(os.listdir(lidar_path))\n","    lidar_data = []\n","\n","    for lidar_file in tqdm(lidar_files, desc=f\"Carregando LiDAR da sequ√™ncia {sequence}\"):\n","        bin_path = os.path.join(lidar_path, lidar_file).replace(\"\\\\\", \"/\")\n","        points = np.fromfile(bin_path, dtype=np.float32).reshape(-1, 4)  # (X, Y, Z, Intensidade)\n","\n","        # Normaliza XYZ para intervalo [-1, 1] com base no valor m√°ximo absoluto\n","        max_abs = np.max(np.abs(points[:, :3]))\n","        points[:, :3] = points[:, :3] / max_abs if max_abs > 0 else points[:, :3]\n","\n","        lidar_data.append(points[:, :3])  # Mant√©m apenas XYZ\n","\n","    return lidar_data  # Lista de numpy arrays"],"metadata":{"id":"udhG_S5AhFhi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Fun√ß√£o de Load de dados de posi√ß√£o (ground truth)"],"metadata":{"id":"X2Q8FDJfhH6b"}},{"cell_type":"code","source":["# Fun√ß√£o para carregar poses (matrizes de transforma√ß√£o 4x4)\n","def load_poses(sequence):\n","    pose_file = os.path.join(BASE_POSES, f\"{sequence}.txt\").replace(\"\\\\\", \"/\")\n","    poses = []\n","\n","    with open(pose_file, \"r\") as f:\n","        for line in f.readlines():\n","            pose_values = np.array([float(x) for x in line.strip().split()]).reshape(3, 4)\n","            pose_matrix = np.vstack((pose_values, [0, 0, 0, 1]))  # Adiciona linha para matriz 4x4\n","            poses.append(pose_matrix)\n","\n","    return np.array(poses)  # Retorna array (N, 4, 4)"],"metadata":{"id":"7P9QYNx0hMys"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Fun√ß√£o para salvar os dados aos poucos"],"metadata":{"id":"OhK5eaoMhROg"}},{"cell_type":"code","source":["# Fun√ß√£o para salvar os dados processados\n","def save_data_partially(train_data, test_data):\n","    with open(os.path.join(SAVE_PATH, \"train_data.pkl\").replace(\"\\\\\", \"/\"), \"wb\") as f:\n","        pickle.dump(train_data, f)\n","    with open(os.path.join(SAVE_PATH, \"test_data.pkl\").replace(\"\\\\\", \"/\"), \"wb\") as f:\n","        pickle.dump(test_data, f)\n","    print(\"‚úÖ Dados salvos com sucesso!\")"],"metadata":{"id":"1nGWmi7XhU83"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Salvar os dados separados de Train e Test por cada sequencia"],"metadata":{"id":"kNQj0LY4hXBV"}},{"cell_type":"code","source":["# Carregar e dividir os dados em treino e teste, salvando progressivamente\n","def load_and_split_data():\n","    train_data = {}\n","    test_data = {}\n","\n","    for seq in range(11):  # KITTI tem sequ√™ncias de 00 a 10\n","        sequence = f\"{seq:02d}\"\n","        print(f\"\\nüîπ Processando sequ√™ncia {sequence}...\")\n","\n","        images = load_images(sequence)\n","        lidar_data = load_lidar(sequence)\n","        poses = load_poses(sequence)\n","\n","        # Separa√ß√£o em treino (00-07) e teste (08-10)\n","        if seq <= 7:\n","            train_data[sequence] = {\"images\": images, \"lidar\": lidar_data, \"poses\": poses}\n","        else:\n","            test_data[sequence] = {\"images\": images, \"lidar\": lidar_data, \"poses\": poses}\n","\n","    save_data_partially(train_data, test_data)"],"metadata":{"id":"H9VBonFnhbOS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Salvar os dados separados direto de Train e Test"],"metadata":{"id":"bSiGqGBThqG7"}},{"cell_type":"code","source":["# Fun√ß√£o para carregar os dados processados\n","def load_saved_data():\n","    train_path = os.path.join(SAVE_PATH, \"train_data.pkl\").replace(\"\\\\\", \"/\")\n","    test_path = os.path.join(SAVE_PATH, \"test_data.pkl\").replace(\"\\\\\", \"/\")\n","\n","    if os.path.exists(train_path) and os.path.exists(test_path):\n","        with open(train_path, \"rb\") as f:\n","            train_data = pickle.load(f)\n","        with open(test_path, \"rb\") as f:\n","            test_data = pickle.load(f)\n","        print(\"\\n‚úÖ Dados carregados com sucesso!\")\n","        return train_data, test_data\n","    else:\n","        print(\"\\n‚ö†Ô∏è Dados n√£o encontrados. Processando novamente...\")\n","        return None, None"],"metadata":{"id":"iMlJiqoCh4Vq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Rodando o pr√©-processamento"],"metadata":{"id":"WX93ygBeh-Iy"}},{"cell_type":"code","source":["# Carregar os dados se j√° existirem, sen√£o processar e salvar\n","train_data, test_data = load_saved_data()\n","\n","if train_data is None or test_data is None:\n","    load_and_split_data()"],"metadata":{"id":"E3oEue9oh9Xs"},"execution_count":null,"outputs":[]}]}